name: demo-in-readme
on: 
  push:
    branches:
      - "debug/test"
      - "new/tmp"
  pull_request:
    branches:
      - "main"
      - "develop"
    paths-ignore:
      - "docs/**"
      - "**.md"
     
env:
  new_k: $(echo $GITHUB_WORKSPACE |cut -d '/' -f 1-4)/data

jobs:
  dataset-preparation:
    runs-on: [lmtest]
    steps:
    - name: set env
      run: |
        dir_prefix=$(echo $GITHUB_WORKSPACE |cut -d '/' -f 1-4)/data
        echo "NEW_PREFIX=$dir_prefix" >> "$GITHUB_ENV" 
        echo "action_state=yellow" >> "$GITHUB_ENV"
     
    - uses: actions/checkout@v3
      with:
         fetch-depth: 2

    - name: raw-chinese-data
      #env:
         #new_k: "${{env.NEW_PREFIX}}"
      run: |
        source activate internlm-env-test
        echo "show env info"
        echo $GITHUB_WORKSPACE
        echo "${{env.NEW_PREFIX}}"
        files=$(git diff --name-only -r HEAD^1 HEAD)
        echo $files
        if [[ $files =~ "runtime.txt" ]]; then
          echo 'pip install -r requirements/runtime.txt'
        fi
        
        if [[ $files =~ "torch.txt"  ]]; then
          echo 'pip install -r requirements/torch.txt'
        fi
     
        echo "${{ env.action_state }}"
        #echo $new_k
        #echo "var test "
        #echo "${{env.new_k}}"
        #sh ./ci_scripts/data/tokenizer_chinese.sh


  
  load-chat-model-in-hf:
    if: ${{ always() }}
    needs: dataset-preparation
    runs-on: [lmtest]
    steps:
    - uses: actions/checkout@v3

    - name: chat-model-in-hf
      run: |
        #echo ${{ secrets.NEW_PREFIX }}
        echo "${{env.new_k}}"
        source activate internlm-env-test
        sh ci_scripts/data/tmp.sh
        
        
        #srun -p llm2 python ./ci_scripts/model/demo_load_7B_chat_model.py
